{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Activation function and its derivative\n",
    "#Sigmoid function\n",
    "def sigmoidal_function(value):\n",
    "    return 1/(1+np.exp(-value))\n",
    "\n",
    "#derivative of sigmoidal function\n",
    "def derivative_sf(value):\n",
    "    return np.exp(-value)/((1+np.exp(-value))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network ALGORITHM:\n",
    "Initialise small random weights for the input weights, input biases, output weights, and output biases.\n",
    "\n",
    "Run the below two algorithms till the iterations stated.\n",
    "### 1.\tForward Pass Algorithm:\n",
    "Matrix multiplication between input data and input weights with input bias terms added.\n",
    "\n",
    "Activation function applied on each of the elements. Let's call this activations_h\n",
    "\n",
    "Matrix multiplication between activations_h and output weights with output bias terms added.\n",
    "\n",
    "On each element of it apply the activation function. Let's call this activations_o\n",
    "\n",
    "### 2.\tBackward Pass Algorithm:\n",
    "Apply derivative of the activation functions in each of the activation-applied matrices received from the FP Algo.\n",
    "\n",
    "Get the difference or the error between the activations_o and the actual output of the training data.\n",
    "\n",
    "Apply hadamard product between the the difference and the derivation at output node. Let's call this delta_o\n",
    "\n",
    "Now apply matrix multiplication between delta_o and transpose of the output weights\n",
    "\n",
    "Find delta_h by applying hadamard product between the resultant matrix in the line above and the derivative at hidden layer\n",
    "\n",
    "Set a learning rate\n",
    "\n",
    "Update the input weights and biases, and output weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network function\n",
    "#Takes in input matrix = i_matrix,\n",
    "#input weights=i_weights,\n",
    "#input bias=i_bias,\n",
    "#i_weights and i_bias is for the input to hidden layer\n",
    "#output weights=o_weights\n",
    "#output bias=o_bias\n",
    "#o_weights and o_bias is for hidden to output layer\n",
    "def NeuralNetworkFunction(i_matrix,i_weights,i_bias,o_weights,o_bias,actual_output,iterations):\n",
    "    for iterate in range(iterations):\n",
    "        #Forward-Propagation steps \n",
    "        #apply dot product between input matrix and weights add bias\n",
    "        #i_matrix.i_weights+i_bias\n",
    "        i_to_h=np.dot(i_matrix,i_weights)+i_bias\n",
    "        #add activation function to each element of i_to_h\n",
    "        activations_h=sigmoidal_function(i_to_h)\n",
    "        #apply dot product to activations_h and o_weights add bias\n",
    "        h_to_o=np.dot(activations_h,o_weights)+o_bias\n",
    "        #add activation function to each element of h_to_o\n",
    "        activations_o=sigmoidal_function(h_to_o)\n",
    "    \n",
    "    \n",
    "        #Back-Propagation steps \n",
    "        #derivatives at output\n",
    "        derivative_o=derivative_sf(activations_o)\n",
    "        #derivatives at hidden layer\n",
    "        derivative_h=derivative_sf(activations_h)\n",
    "        #difference between the actual output value and results from forward pass\n",
    "        diff=actual_output-activations_o\n",
    "        #delta output = diff *(scalar) derivatives at output\n",
    "        delta_o = diff * derivative_o\n",
    "        #hidden layer difference\n",
    "        diff_hidden=np.dot(delta_o,np.transpose(o_weights))\n",
    "        #delta hidden\n",
    "        delta_h=diff_hidden * derivative_h\n",
    "        #Updating changes after each iteration\n",
    "        #We take learning rate to be 0.1\n",
    "        alpha=0.1\n",
    "        #Updating input weights and biases\n",
    "        i_weights=i_weights+np.dot(np.transpose(i_matrix),delta_h)*alpha\n",
    "        i_bias=i_bias+np.sum(delta_h,axis=0) * alpha\n",
    "        #Updating output weights and biases\n",
    "        o_weights=o_weights+np.dot(np.transpose(activations_h),delta_o)*alpha\n",
    "        o_bias=o_bias+np.sum(delta_o,axis=0) * alpha\n",
    "    #Updating the global weights and biases\n",
    "    #with the one which would yield\n",
    "    #maximum accuracy\n",
    "    global input_weight_matrix\n",
    "    input_weight_matrix=i_weights\n",
    "    global input_bias_matrix\n",
    "    input_bias_matrix=i_bias\n",
    "    global output_weight_matrix\n",
    "    output_weight_matrix=o_weights\n",
    "    global output_bias_matrix\n",
    "    output_bias_matrix=o_bias\n",
    "    return activations_o\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input weights (Input to hidden):\n",
      "\n",
      "[[4.68895786e-03 9.22336416e-05 8.92227521e-03]\n",
      " [5.76187014e-03 5.65580106e-03 1.01676470e-05]]\n",
      "Input bias (bias at input to hidden):\n",
      "\n",
      "[[0.00276843 0.00785333 0.00096587]]\n",
      "Output weight (hidden to output):\n",
      "\n",
      "[[0.00333646]\n",
      " [0.00452083]\n",
      " [0.00060247]]\n",
      "Output bias (bias at hidden to output):\n",
      "\n",
      "[[0.00357114]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize input matrix\n",
    "import pandas as pan\n",
    "#Loading the moon dataset\n",
    "moon_dataset = pan.read_csv('Put the path name')\n",
    "#Converting it into a DataFrame\n",
    "moon_dataset = pan.DataFrame(moon_dataset)\n",
    "#Taking a sample to be the training set\n",
    "moon_train = moon_dataset.sample(280, replace = False)\n",
    "#Rest to be the test data set\n",
    "moon_test = pan.concat([moon_dataset, moon_train, moon_train]).drop_duplicates(keep=False)\n",
    "\n",
    "\n",
    "# Creating an array of input dataset of moon\n",
    "moon_input_train = np.array(moon_train[['X0','X1']])\n",
    "moon_input_test = np.array(moon_test[['X0','X1']])\n",
    "\n",
    "\n",
    "# storing output\n",
    "moon_output_train = np.array(moon_train[['Class']])\n",
    "moon_output_test = np.array(moon_test[['Class']])\n",
    "\n",
    "#No. of columns\n",
    "features_length=len(moon_dataset.columns)-1\n",
    "#No. of nodes at hidden layer\n",
    "nodes_at_hidden_layer= 3\n",
    "#No. of nodes at output layer\n",
    "nodes_at_outer_layer= 1\n",
    "\n",
    "#Filling the following matrices with random values\n",
    "#Will be a (No. of features)x(Nodes at the hidden layer) matrix\n",
    "input_weight_matrix = np.random.rand(features_length,nodes_at_hidden_layer)*0.01\n",
    "#will be a 1x(Nodes at hidden layer) matrix\n",
    "input_bias_matrix = np.random.rand(1,nodes_at_hidden_layer)*0.01\n",
    "#This will be a (No. at hidden layer)x1 matrix\n",
    "output_weight_matrix = np.random.rand(nodes_at_hidden_layer,1)*0.01\n",
    "#1X(Nodes at outer layer) matrix\n",
    "output_bias_matrix = np.random.rand(1,nodes_at_outer_layer)*0.01\n",
    "\n",
    "print(\"Input weights (Input to hidden):\\n\")\n",
    "print(input_weight_matrix)\n",
    "print(\"Input bias (bias at input to hidden):\\n\")\n",
    "print(input_bias_matrix)\n",
    "print(\"Output weight (hidden to output):\\n\")\n",
    "print(output_weight_matrix)\n",
    "print(\"Output bias (bias at hidden to output):\\n\")\n",
    "print(output_bias_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\suchandra\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#i_matrix,i_weights,i_bias,o_weights,o_bias,actual_output,iterations\n",
    "#Gets the result of the training data\n",
    "result_train = NeuralNetworkFunction(moon_input_train,input_weight_matrix,input_bias_matrix,output_weight_matrix,output_bias_matrix,moon_output_train,7000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetworkTest function\n",
    "This function uses just the single forward-propagation algorithm\n",
    "with the updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The function NeuralNetworkTest which calculates the activations for the test_images in one pass\n",
    "def NeuralNetworkTest(i_matrix,i_weights,i_bias,o_weights,o_bias):\n",
    "    #apply dot product between input matrix and weights add bias\n",
    "    #i_matrix.i_weights+i_bias\n",
    "    i_to_h=np.dot(i_matrix,i_weights)+i_bias\n",
    "    #add activation function to each element of i_to_h\n",
    "    activations_h=sigmoidal_function(i_to_h)\n",
    "    #apply dot product to activations_h and o_weights add bias\n",
    "    h_to_o=np.dot(activations_h,o_weights)+o_bias\n",
    "    #add activation function to each element of h_to_o\n",
    "    activations_o=sigmoidal_function(h_to_o)\n",
    "    return activations_o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Function\n",
    "This function takes in a data and checks if it is > 0.5, if it is so,then classifies it as 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Range above 0.5 is classified as 1.0\n",
    "def classifier(matrix_data):\n",
    "    for i in range(len(matrix_data[:,0])):\n",
    "        if(matrix_data[i]>=0.5):\n",
    "            matrix_data[i]=1.0\n",
    "        else:\n",
    "            matrix_data[i]=0.0\n",
    "    return matrix_data\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below prints the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\suchandra\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the test data with the weights\n",
    "testing0 = NeuralNetworkTest(moon_input_test,input_weight_matrix,input_bias_matrix,output_weight_matrix,output_bias_matrix)\n",
    "testing0=classifier(testing0)\n",
    "count=0\n",
    "for i in range(len(moon_output_test[:,0])):\n",
    "    if((moon_output_test[i]/1.0)==testing0[i]):\n",
    "        count = count+1\n",
    "count/len(testing0[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### For MNIST, we have used tensorflow to read the exmaples of train and test datasets\n",
    "# Using one hot encoder as 'False' will retain the digital value of the label instead of encoding it as binary\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the type of data being handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28283a04cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To view the dataset's first digit using matplotlib library in python\n",
    "import matplotlib.pyplot as plot\n",
    "example = mnist_data.train.next_batch(1) # Taking a batch from train dataset in mnist\n",
    "view = example[0] \n",
    "view = view.reshape(28, 28) # reshaping the 784 columns as 28x28 image format to be plotted\n",
    "plot.gray() # plotting in gray plot\n",
    "plot.imshow(view)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data images and labels\n",
    "train_images = mnist_data.train.images\n",
    "train_labels = mnist_data.train.labels\n",
    "\n",
    "# test data images and label\n",
    "test_images = mnist_data.test.images\n",
    "test_labels = mnist_data.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINARY CLASSIFICATION\n",
    "Since we have to distinguish between 1 and 7. We make one of the inputs as 0 with this \n",
    "### makeZero function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function called makeZero is defined below which will be used later to convert all train and test labels of 7 to 0 \n",
    "# for binary classification\n",
    "\n",
    "def makeZero(input):\n",
    "    for i in range(len(input)):\n",
    "        input[i]=0\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of 1s and 7s from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we subset the 60k training examples to extract the 2 digits of interest. In our case, its 1 and 7\n",
    "## Getting training and test images for digits 1 and 7\n",
    "## sklearn utils has been used only to use shuffle function in order to shuffle the training and testing datasets\n",
    "# Since the datasets for training and testing are different, I have first extracted the indices from labels datasets for \n",
    "# both train and test and then used the indices to extract the images.\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# extracts indices from train labels dataset where labels = 1\n",
    "one_index_train = np.where(train_labels == 1) \n",
    "seven_index_train = np.where(train_labels == 7)\n",
    "\n",
    "# extracts all the images at the indices obtained earlier for 1 and 7\n",
    "image_one_train = train_images[one_index_train] \n",
    "image_seven_train = train_images[seven_index_train]\n",
    "\n",
    "# extracts the labels for 1 and 7 \n",
    "one_train_labels = train_labels[train_labels == 1]  \n",
    "seven_train_labels = train_labels[train_labels == 7]\n",
    "\n",
    "# Now we use the previous makeZero function defined to make all 7 labels in train to 0\n",
    "seven_train_labels=makeZero(seven_train_labels)\n",
    "\n",
    "# Concatenating the training images and labels for 1 and 7 by rows\n",
    "train_images = np.concatenate((image_one_train,image_seven_train),axis=0)\n",
    "train_labels = np.concatenate((one_train_labels,seven_train_labels),axis=0)\n",
    "\n",
    "# Randomize data point by using shuffle to reorganize the images\n",
    "train_images, train_labels = shuffle(train_images, train_labels)\n",
    "\n",
    "\n",
    "# Repeating the above process for the test datasets. Since the other digit is already 1, no encoding is required.\n",
    "one_index_test = np.where(test_labels == 1)\n",
    "seven_index_test = np.where(test_labels == 7)\n",
    "\n",
    "image_one_test = test_images[one_index_test]\n",
    "image_seven_test = test_images[seven_index_test]\n",
    "\n",
    "one_test_labels = test_labels[test_labels == 1]\n",
    "seven_test_labels = test_labels[test_labels == 7]\n",
    "\n",
    "seven_test_labels=makeZero(seven_test_labels)\n",
    "\n",
    "test_images = np.concatenate((image_one_test,image_seven_test),axis=0)\n",
    "test_labels = np.concatenate((one_test_labels,seven_test_labels),axis=0)\n",
    "\n",
    "test_images, test_labels = shuffle(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the number of nodes, weights, and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pan\n",
    "import random as rand\n",
    "\n",
    "print(train_labels[0])\n",
    "\n",
    "# Reshaping the labels as a matrix of 11894 rows and 1 column which has the labels \n",
    "train_labels = train_labels.reshape(11894,1)\n",
    "\n",
    "# Initialising the variables\n",
    "features_length= 784 # features_length is number of columns\n",
    "nodes_at_hidden_layer= 3 # setting number of nodes we want in our hidden layer\n",
    "nodes_at_outer_layer= 1 # Single output node\n",
    "\n",
    "# Here, 0.01 is the learning rate 'alpha'\n",
    "input_weight_matrix = np.random.rand(features_length,nodes_at_hidden_layer)* 0.01\n",
    "input_bias_matrix = np.random.rand(1,nodes_at_hidden_layer)* 0.01\n",
    "output_weight_matrix = np.random.rand(nodes_at_hidden_layer,1)* 0.01\n",
    "output_bias_matrix = np.random.rand(1,nodes_at_outer_layer)* 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the mnist dataset (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\suchandra\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "142.0084843635559"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training our neural network defined by giving the input variables required. Also using time to check the duration \n",
    "# for the algorithm to execute\n",
    "import time\n",
    "start = time.time()\n",
    "result_train = NeuralNetworkFunction(train_images,input_weight_matrix,input_bias_matrix,output_weight_matrix,output_bias_matrix,train_labels,1000)\n",
    "stop = time.time()\n",
    "duration = stop - start\n",
    "duration\n",
    "\n",
    "## For batch gradient descent the time taken is 142.01 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we store the activations of result_train in a variable and if the confidence for an image by our learner is \n",
    "# greater than or equal to 0.5, its taken to be 1, else 0\n",
    "\n",
    "data_result = result_train\n",
    "\n",
    "data_result.dtype # checking the dtype of varible before looping over\n",
    "\n",
    "\n",
    " \n",
    "data_result=classifier(data_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.52917437363377"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we define a function called accuracy which will be used to calculate our algorithm's accuracy rate in learning as well as\n",
    "# testing. we got 99.52% accuracy in training\n",
    "def accuracy(data_result, accurate):\n",
    "    count=0\n",
    "    for i in range(len(data_result[:,0])):\n",
    "        if(data_result[i] == accurate[i]/1.0):\n",
    "            count = count+1\n",
    "    return count/len(data_result[:,0]) * 100\n",
    "\n",
    "accuracy(data_result, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing mnist\n",
    "# Converting test_labels to a matrix with 2163 rows and 1 columns. These 2163 rows have test labels for only digits 1 and 7\n",
    "# encoded as 1 and 0 respectively using makeZero above\n",
    "test_labels = test_labels.reshape(2163,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained weights to test on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\suchandra\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# test our neural network to return activations for the test set\n",
    "testingMNIST = NeuralNetworkTest(test_images,input_weight_matrix,input_bias_matrix,output_weight_matrix,output_bias_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.30651872399446"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the classifier function again to convert the activations output\n",
    "testingMNIST = classifier(testingMNIST)\n",
    "\n",
    "# check accuracy for the test_labels\n",
    "# We got test accuracy of 99.31%\n",
    "accuracy(testingMNIST, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4 IMPLEMENTION \n",
    "### MiniBatchNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since in MNIST dataset, we have until now used batch gradient descent which gives us good results but takes an \n",
    "# immense time to execute; thereby, one enhancement is to use mini batch gradient descent.\n",
    "# Since the training set is 11894 rows, at first i have decided to take batches of 100 and last batch will have 94 data rows\n",
    "#119 iterations required\n",
    "\n",
    "# making a global variable error to store error in classification\n",
    "global error\n",
    "error = []\n",
    "def MiniBatchNN(input_matrix,input_weights,input_bias,output_weights,output_bias,actual_output):\n",
    "    \n",
    "    \n",
    "    # Starting with 10 overall iterations on whole dataset.\n",
    "    # I tried out 100 iterations as well but the accuracy improved by only 0.05% which didn't seem significant. Thus \n",
    "    # overall 10 iterations seem enough\n",
    "    for i in range(0, 10):\n",
    "        iterations = 19 # no of iterations required to go through entire dataset taking batches of 626 at a time\n",
    "        offset = 0 # offset variables\n",
    "        end = 626  # decides the limit of minibatch size\n",
    "        \n",
    "        for j in range(iterations):\n",
    "            train_input = train_images[offset:end] # defining the minibatch train input\n",
    "            train_out = train_labels[offset : end] # extracting the labels of minibatch size\n",
    "            \n",
    "            #apply dot product between input matrix and weights add bias i_matrix.i_weights+i_bias\n",
    "            i_to_h = np.dot(input_matrix,input_weights)+input_bias\n",
    "            \n",
    "            #add activation function to each element of i_to_h\n",
    "            activations_h = sigmoidal_function(i_to_h)\n",
    "            \n",
    "            #apply dot product to activations_h and o_weights add bias\n",
    "            h_to_o = np.dot(activations_h,output_weights)+output_bias\n",
    "            \n",
    "            #add activation function to each element of h_to_o\n",
    "            activations_o = sigmoidal_function(h_to_o)\n",
    "            #print(activations_o.shape)\n",
    "\n",
    "            #derivatives at output\n",
    "            derivative_o = derivative_sf(activations_o)\n",
    "            #print(derivative_o.shape)\n",
    "            \n",
    "            #derivatives at hidden layer\n",
    "            derivative_h = derivative_sf(activations_h)\n",
    "            #print(derivative_h.shape)\n",
    "            \n",
    "            #difference between the actual output value and results from forward pass\n",
    "            diff = actual_output-activations_o\n",
    "            #print(diff.shape)\n",
    "            #print(diff.shape)\n",
    "            \n",
    "            #delta output = diff *(scalar) derivatives at output\n",
    "            delta_o = diff * derivative_o\n",
    "            #print(delta_o.shape)\n",
    "            \n",
    "            #hidden layer difference\n",
    "            diff_hidden=np.dot(delta_o,np.transpose(output_weights))\n",
    "            \n",
    "            #delta hidden\n",
    "            delta_h=diff_hidden * derivative_h\n",
    "            \n",
    "            #Updating changes after each iteration\n",
    "            #We take learning rate to be 0.01\n",
    "            alpha=0.01\n",
    "            \n",
    "            #Updating input weights and biases\n",
    "            input_weights = input_weights+np.dot(np.transpose(input_matrix),delta_h)*alpha\n",
    "            input_bias = input_bias+np.sum(delta_h,axis=0) * alpha\n",
    "            \n",
    "            #Updating output weights and biases\n",
    "            output_weights=output_weights+np.dot(np.transpose(activations_h),delta_o)*alpha\n",
    "            output_bias=output_bias+np.sum(delta_o,axis=0) * alpha\n",
    "            offset = offset + 626\n",
    "            end = end + 626\n",
    "        error.append(sum(diff)/11894) # saving errors per iteration\n",
    "    \n",
    "    \n",
    "    # storing the below variables as global so as to be used later throughout the code\n",
    "    global input_weight_matrix\n",
    "    input_weight_matrix=input_weights\n",
    "    global input_bias_matrix\n",
    "    input_bias_matrix=input_bias\n",
    "    global output_weight_matrix\n",
    "    output_weight_matrix=output_weights\n",
    "    global output_bias_matrix\n",
    "    output_bias_matrix=output_bias\n",
    "    return activations_o\n",
    "\n",
    "\n",
    "# Reshaping train labels and setting the other input variables\n",
    "train_labels = train_labels.reshape(11894,1)\n",
    "\n",
    "\n",
    "features_length= 784\n",
    "nodes_at_hidden_layer= 3\n",
    "nodes_at_outer_layer= 1\n",
    "\n",
    "input_weight_matrix = np.random.rand(features_length,nodes_at_hidden_layer)*0.01\n",
    "input_bias_matrix = np.random.rand(1,nodes_at_hidden_layer)*0.01\n",
    "output_weight_matrix = np.random.rand(nodes_at_hidden_layer,1)*0.01\n",
    "output_bias_matrix = np.random.rand(1,nodes_at_outer_layer)*0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of runtime duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\suchandra\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.748470306396484\n"
     ]
    }
   ],
   "source": [
    "## To compare previous algorithm of batch gradient descent with minibatch gradient descent, we use time library\n",
    "# to time the execution and compare\n",
    "start = time.time()\n",
    "result_train = MiniBatchNN(train_images,input_weight_matrix,input_bias_matrix,output_weight_matrix,output_bias_matrix,train_labels)\n",
    "stop = time.time()   \n",
    "duration = stop-start\n",
    "print(duration)\n",
    "\n",
    "# Here the duration shows 27.75 s. \n",
    "## In the previous case, for batch gradient descent we saw that the time taken for execution was 143.25 secs. Hence, \n",
    "# mini gradient descent makes the training process close to 5 times faster with 1% difference in accuracy. Another point to\n",
    "# notice is it takes far lesser iterations for minibatch to converge to a close to optimal solution comapred to batch. \n",
    "# In batch gradient descent, we have 1000 iterations over the whole dataset, whereas in minibatch we had 10 iterations for \n",
    "# the whole batch and more sub iterations to learn from various batches that we divide the data into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\suchandra\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.6130374479889"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting activations to 0 and 1 based on confidence and checking accuracy for train dataset\n",
    "classifier(result_train)\n",
    "\n",
    "accuracy(result_train, train_labels)\n",
    "\n",
    "# We get 98.48% accuracy for train dataset\n",
    "\n",
    "# using the minibatch gradient algorithm to test accuracy on test dataset\n",
    "testingMNIST = NeuralNetworkTest(test_images,input_weight_matrix,input_bias_matrix,output_weight_matrix,output_bias_matrix)\n",
    "testingMNIST = classifier(testingMNIST)\n",
    "\n",
    "accuracy(testingMNIST, test_labels)\n",
    "# We get 98.61% accuracy in testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error)\n",
    "\n",
    "p = pan.DataFrame(error, pan.Series(range(0,10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28282b90828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = p[0:]\n",
    "x = pan.Series(range(0,10))\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.000063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "1 -0.001671\n",
       "2 -0.001257\n",
       "3 -0.000868\n",
       "4 -0.000012\n",
       "5 -0.000168\n",
       "6 -0.000195\n",
       "7 -0.000109\n",
       "8 -0.000145\n",
       "9 -0.000063"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input weights (Input to hidden):\n",
      "\n",
      "[[0.0005477  0.00601693 0.00844977]\n",
      " [0.00361427 0.00569035 0.00680072]]\n",
      "Input bias (bias at input to hidden):\n",
      "\n",
      "[[0.00757086 0.0047338  0.00418174]]\n",
      "Output weight (hidden to output):\n",
      "\n",
      "[[0.00410232]\n",
      " [0.00396452]\n",
      " [0.00404439]]\n",
      "Output bias (bias at hidden to output):\n",
      "\n",
      "[[0.00349287]]\n"
     ]
    }
   ],
   "source": [
    "#This has been worked on with the moon dataset\n",
    "#Initialize input matrix\n",
    "\n",
    "import pandas as pan\n",
    "#Loading the moon dataset\n",
    "moon_dataset = pan.read_csv('C:/Users/Suchandra/Documents/SEM2/ADV ML and IR/ASSIGNMENTS ML/moons400.csv')\n",
    "#Converting it into a DataFrame\n",
    "moon_dataset = pan.DataFrame(moon_dataset)\n",
    "#Taking a sample to be the training set\n",
    "moon_train = moon_dataset.sample(280, replace = False)\n",
    "#Rest to be the test data set\n",
    "moon_test = pan.concat([moon_dataset, moon_train, moon_train]).drop_duplicates(keep=False)\n",
    "\n",
    "\n",
    "# Creating an array of input dataset of moon\n",
    "moon_input_train = np.array(moon_train[['X0','X1']])\n",
    "moon_input_test = np.array(moon_test[['X0','X1']])\n",
    "\n",
    "\n",
    "# storing output\n",
    "moon_output_train = np.array(moon_train[['Class']])\n",
    "moon_output_test = np.array(moon_test[['Class']])\n",
    "\n",
    "#No. of columns\n",
    "features_length=len(moon_dataset.columns)-1\n",
    "#No. of nodes at hidden layer\n",
    "nodes_at_hidden_layer= 3\n",
    "#No. of nodes at output layer\n",
    "nodes_at_outer_layer= 1\n",
    "\n",
    "#Filling the following matrices with random values\n",
    "#Will be a (No. of features)x(Nodes at the hidden layer) matrix\n",
    "input_weight_matrix = np.random.rand(features_length,nodes_at_hidden_layer)*0.01\n",
    "#will be a 1x(Nodes at hidden layer) matrix\n",
    "input_bias_matrix = np.random.rand(1,nodes_at_hidden_layer)*0.01\n",
    "#This will be a (No. at hidden layer)x1 matrix\n",
    "output_weight_matrix = np.random.rand(nodes_at_hidden_layer,1)*0.01\n",
    "#1X(Nodes at outer layer) matrix\n",
    "output_bias_matrix = np.random.rand(1,nodes_at_outer_layer)*0.01\n",
    "\n",
    "print(\"Input weights (Input to hidden):\\n\")\n",
    "print(input_weight_matrix)\n",
    "print(\"Input bias (bias at input to hidden):\\n\")\n",
    "print(input_bias_matrix)\n",
    "print(\"Output weight (hidden to output):\\n\")\n",
    "print(output_weight_matrix)\n",
    "print(\"Output bias (bias at hidden to output):\\n\")\n",
    "print(output_bias_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracyMeasure function\n",
    "In the below cell, we implement an early stop algorithm\n",
    "Where a limit to the no. of iterations is set (here, it is 2000)\n",
    "After each iteration accuracy is calculated\n",
    "If the accuracy is greater than the maximum stored accuracy in the list\n",
    "The algorithm continues\n",
    "else it stops\n",
    "At last the final weights with the maximum accuracy is stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.8642857142857143\n",
      "110 0.8642857142857143\n",
      "120 0.8785714285714286\n",
      "130 0.8785714285714286\n",
      "140 0.8785714285714286\n",
      "input_weight_matrix:\n",
      " [[ -7.89652947 -11.38848143 -13.82670996]\n",
      " [ 27.06893126  37.65549957  44.65963333]]\n",
      "input_bias_matrix:\n",
      " [[-1.55613565 -1.21317764 -1.05312838]]\n",
      "output_weight_matrix:\n",
      " [[-0.73226729]\n",
      " [-1.45338655]\n",
      " [-1.94323667]]\n",
      "input_bias_matrix:\n",
      " [[2.08486331]]\n"
     ]
    }
   ],
   "source": [
    "#Here, we implement an early stop algorithm\n",
    "#Where a limit to the no. of iterations is set (here, it is 2000)\n",
    "#After each iteration accuracy is calculated\n",
    "#If the accuracy is greater than the maximum stored accuracy in the list\n",
    "#The algorithm continues\n",
    "#else it stops\n",
    "#At last the final weights with the maximum accuracy is stored  \n",
    "\n",
    "moon_output_train=moon_output_train/1.0\n",
    "#We have an empty accuracy list storing the accuracies. \n",
    "standardAccuracy=[]\n",
    "#Initialise the list\n",
    "standardAccuracy.append(0.0)\n",
    "def accuracyMeasure():\n",
    "    #Initial start of iterations\n",
    "    iterations=100\n",
    "    #Initialise accuracy to be 0.0\n",
    "    accuracy=0.0\n",
    "    while(iterations<=2000):\n",
    "        #This returns the weights and biases \n",
    "        WeightsNN=input_weight_matrix,input_bias_matrix,output_weight_matrix,output_bias_matrix\n",
    "        #Trains the neural net\n",
    "        result_train = NeuralNetworkFunction(moon_input_train,input_weight_matrix,input_bias_matrix,output_weight_matrix,output_bias_matrix,moon_output_train,iterations)\n",
    "        #Changes the output result\n",
    "        def classifier(matrix_data):\n",
    "            for i in range(len(matrix_data[:,0])):\n",
    "                if(matrix_data[i]>=0.5):\n",
    "                    matrix_data[i]=1\n",
    "                else:\n",
    "                    matrix_data[i]=0\n",
    "            return matrix_data\n",
    "        #Invoking the classifier function\n",
    "        result_train=classifier(result_train)\n",
    "        #Initialising count variable to zero\n",
    "        count=0\n",
    "        for i in range(len(moon_output_train[:,0])):\n",
    "            #compares the y to ycap, i.e if \n",
    "            #the predicted score matches\n",
    "            #actual score, count is increased by 1\n",
    "            if(moon_output_train[i]==result_train[i]):\n",
    "                count = count+1\n",
    "        #checks the accuracy\n",
    "        accuracy=count/len(result_train[:,0])\n",
    "        global standardAccuracy\n",
    "        #if the current accuracy is greater than the one in the list\n",
    "        #iterations are increased by 10\n",
    "        #the new accuracy is added to the standardAccuracy list\n",
    "        if(accuracy>=max(standardAccuracy)):\n",
    "            print(iterations,accuracy)\n",
    "            iterations=iterations+10\n",
    "            standardAccuracy.append(accuracy)\n",
    "        else:\n",
    "            #else iterations is set to more than max\n",
    "            #to terminate the function\n",
    "            iterations=2001\n",
    "    #returns the weights\n",
    "    #WeightsNN[0] for input_weight_matrix\n",
    "    #WeightsNN[1] for input_bias_matrix\n",
    "    #WeightsNN[2] for output_weight_matrix\n",
    "    #WeightsNN[3] for output_bias_matrix\n",
    "    return WeightsNN\n",
    "\n",
    "AM=accuracyMeasure()\n",
    "print(\"input_weight_matrix:\\n\",AM[0])\n",
    "print(\"input_bias_matrix:\\n\",AM[1])\n",
    "print(\"output_weight_matrix:\\n\",AM[2])\n",
    "print(\"input_bias_matrix:\\n\",AM[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph showing the accuracy vs. Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28284199630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This plots the accuracy by iterations graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "y = standardAccuracy\n",
    "x = pan.Series(range(0,len(standardAccuracy)))\n",
    "\n",
    "plt.plot((x+10)*10, y)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the test data with the weights\n",
    "testing = NeuralNetworkTest(moon_input_test,AM[0],AM[1],AM[2],AM[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "testing=classifier(testing)\n",
    "print(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy after the enhancement\n",
    "This function (accuracyMeasure) enables the Neural network to reach a good accuracy in less number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8833333333333333"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compares the accuracy; if the output by Neural network matches the actual output \n",
    "#count is increased by 1\n",
    "\n",
    "count=0\n",
    "for i in range(len(moon_output_test[:,0])):\n",
    "    if((moon_output_test[i]/1.0)==testing[i]):\n",
    "        count = count+1\n",
    "count/len(testing[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "##### 1. Andrew Ng Lectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
